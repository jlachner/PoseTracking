import cv2
import mediapipe as mp
import numpy as np
import toml
import pandas as pd

def draw_right_arm_landmarks(rgb_image, pose_results):
    annotated_image = np.copy(rgb_image)
    
    # Initialize drawing specs
    landmark_drawing_spec = mp.solutions.drawing_utils.DrawingSpec(color=(83, 88, 93), thickness=8, circle_radius=10)
    connection_drawing_spec = mp.solutions.drawing_utils.DrawingSpec(color=(255, 88, 0), thickness=8, circle_radius=10) # orange
    
    # Right arm connections based on MediaPipe Pose landmark numbering
    right_arm_connections = [
        (12, 14),  # Right shoulder to right elbow
        (14, 16),  # Right elbow to right wrist
    ]
    
    if pose_results.pose_landmarks:
        # Draw each connection
        for connection in right_arm_connections:
            start_idx, end_idx = connection
            start_landmark = pose_results.pose_landmarks.landmark[start_idx]
            end_landmark = pose_results.pose_landmarks.landmark[end_idx]
            cv2.line(annotated_image, 
                     (int(start_landmark.x * annotated_image.shape[1]), int(start_landmark.y * annotated_image.shape[0])), 
                     (int(end_landmark.x * annotated_image.shape[1]), int(end_landmark.y * annotated_image.shape[0])), 
                     connection_drawing_spec.color, connection_drawing_spec.thickness)
        
        # Draw each landmark
        for idx in [12, 14, 16]:  # Right shoulder, elbow, and wrist
            landmark = pose_results.pose_landmarks.landmark[idx]
            cv2.circle(annotated_image, 
                       (int(landmark.x * annotated_image.shape[1]), int(landmark.y * annotated_image.shape[0])), 
                       landmark_drawing_spec.circle_radius, landmark_drawing_spec.color, landmark_drawing_spec.thickness)
    
    return annotated_image

def get_best_landmarks(landmarks_lists, confidences_lists, landmark_indices):
    best_landmarks = []

    for idx in landmark_indices:
        best_confidence = -1
        best_landmark = None
        for landmarks, confidences in zip(landmarks_lists, confidences_lists):
            if confidences[idx] > best_confidence:
                best_confidence = confidences[idx]
                best_landmark = landmarks[idx]
        best_landmarks.append({'x': best_landmark.x, 'y': best_landmark.y, 'z': best_landmark.z})

    return best_landmarks

# File paths
input_video_paths = [
    '/Users/johanneslachner/Documents/InMotion_Tracking/Input/subject1/wrist/Camera_000_synchronized.mp4',
    '/Users/johanneslachner/Documents/InMotion_Tracking/Input/subject1/wrist/Camera_001_synchronized.mp4',
    '/Users/johanneslachner/Documents/InMotion_Tracking/Input/subject1/wrist/Camera_002_synchronized.mp4'
]
output_video_paths = [
    '/Users/johanneslachner/Documents/InMotion_Tracking/Output/subject1/wrist/annotated_video0.mp4',
    '/Users/johanneslachner/Documents/InMotion_Tracking/Output/subject1/wrist/annotated_video1.mp4',
    '/Users/johanneslachner/Documents/InMotion_Tracking/Output/subject1/wrist/annotated_video2.mp4'
]
best_landmarks_csv_path = '/Users/johanneslachner/Documents/InMotion_Tracking/Output/subject1/wrist/best_landmarks.csv'

# Initialize MediaPipe Pose
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(model_complexity=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)

# Initialize video captures and writers
caps = [cv2.VideoCapture(path) for path in input_video_paths]
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
fps = caps[0].get(cv2.CAP_PROP_FPS)
frame_width = int(caps[0].get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(caps[0].get(cv2.CAP_PROP_FRAME_HEIGHT))
outs = [cv2.VideoWriter(path, fourcc, fps, (frame_width, frame_height)) for path in output_video_paths]

# Prepare a list to store the best 3D points
best_landmarks_list = []
landmark_indices = [12, 14, 16]  # Indices for right shoulder, elbow, and wrist

while all(cap.isOpened() for cap in caps):
    rets, frames = zip(*(cap.read() for cap in caps))
    
    if not all(rets):
        break

    images_rgb = [cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) for frame in frames]
    pose_results = [pose.process(image_rgb) for image_rgb in images_rgb]

    if all(result.pose_landmarks for result in pose_results):
        landmarks_lists = [result.pose_landmarks.landmark for result in pose_results]
        confidences_lists = [[landmark.visibility for landmark in result.pose_landmarks.landmark] for result in pose_results]

        best_landmarks = get_best_landmarks(landmarks_lists, confidences_lists, landmark_indices)
        best_landmarks_list.append(best_landmarks)

    annotated_images = [draw_right_arm_landmarks(image_rgb, result) for image_rgb, result in zip(images_rgb, pose_results)]
    annotated_images_bgr = [cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR) for annotated_image in annotated_images]

    for out, annotated_image_bgr in zip(outs, annotated_images_bgr):
        out.write(annotated_image_bgr)

    cv2.imshow('MediaPipe Pose Video 1', annotated_images_bgr[0])
    cv2.imshow('MediaPipe Pose Video 2', annotated_images_bgr[1])
    cv2.imshow('MediaPipe Pose Video 3', annotated_images_bgr[2])

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release everything
for cap in caps:
    cap.release()
for out in outs:
    out.release()
cv2.destroyAllWindows()

# Create a DataFrame and save the best landmarks to a CSV file
# Flatten the list of best landmarks to create rows for the DataFrame
flattened_best_landmarks = [
    {
        'shoulder_x': landmarks[0]['x'], 'shoulder_y': landmarks[0]['y'], 'shoulder_z': landmarks[0]['z'],
        'elbow_x': landmarks[1]['x'], 'elbow_y': landmarks[1]['y'], 'elbow_z': landmarks[1]['z'],
        'wrist_x': landmarks[2]['x'], 'wrist_y': landmarks[2]['y'], 'wrist_z': landmarks[2]['z'],
    }
    for landmarks in best_landmarks_list
]

df_best_landmarks = pd.DataFrame(flattened_best_landmarks)
df_best_landmarks.to_csv(best_landmarks_csv_path, index=False)